{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import (Sequence,Lowercase, NFD, StripAccents)\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.decoders import BPEDecoder\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В 2003-ем году под руководством малоизвестного...</td>\n",
       "      <td>0</td>\n",
       "      <td>dataset/neg/1000083-0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Грустно и печально. Грустно от того, что довол...</td>\n",
       "      <td>0</td>\n",
       "      <td>dataset/neg/1000083-1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Давным-давно Кира Найтли ворвалась на экран от...</td>\n",
       "      <td>0</td>\n",
       "      <td>dataset/neg/1000125-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Я, в общем, ничего против уравновешенного феми...</td>\n",
       "      <td>0</td>\n",
       "      <td>dataset/neg/1000125-4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Измена  один из сюжетов, который всегда будет ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dataset/neg/1000125-6.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  В 2003-ем году под руководством малоизвестного...      0   \n",
       "1  Грустно и печально. Грустно от того, что довол...      0   \n",
       "2  Давным-давно Кира Найтли ворвалась на экран от...      0   \n",
       "3  Я, в общем, ничего против уравновешенного феми...      0   \n",
       "4  Измена  один из сюжетов, который всегда будет ...      0   \n",
       "\n",
       "                        link  \n",
       "0  dataset/neg/1000083-0.txt  \n",
       "1  dataset/neg/1000083-1.txt  \n",
       "2  dataset/neg/1000125-3.txt  \n",
       "3  dataset/neg/1000125-4.txt  \n",
       "4  dataset/neg/1000125-6.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size:30000\n"
     ]
    }
   ],
   "source": [
    "special_tokens=[\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"]\n",
    "\n",
    "temp_proc= TemplateProcessing(\n",
    "                single = \"[CLS] $A [SEP]\",\n",
    "                pair = \"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "                special_tokens = [\n",
    "                            (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "                            (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "                                 ],\n",
    "                )\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.normalizer = Sequence(\n",
    "                                [NFD(), # нормализатор Unicode\n",
    "                                Lowercase(), # нижний регистр\n",
    "                                StripAccents()] # удаление доп. буквенных символов (напрмер волну над буквами в турецком языке)\n",
    "                            )\n",
    "tokenizer.pre_tokenizer = Whitespace() # разбиение на пробелы\n",
    "tokenizer.decoder = BPEDecoder() # совместимый с BPE декодер\n",
    "tokenizer.post_processor = temp_proc\n",
    "\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size = 30000,\n",
    "    special_tokens = special_tokens\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(data['text'].to_list(), trainer = trainer)\n",
    "print(f\"Trained vocab size:{tokenizer.get_vocab_size()}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка работоспособности токенизатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простое предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'после', 'этого', 'момента', ',', 'я', 'больше', 'не', 'хочу', 'есть', 'ки', 'ви', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = 'После этого момента, я больше не хочу есть киви'\n",
    "text_encode = tokenizer.encode(text)\n",
    "print(text_encode.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сложные слова, которые токенизатор не знает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ненавижу', 'трансформеры', ',', 'а', 'особенно', 'их', 'ска', 'чивать', 'с', 'h', 'u', 'g', 'g', 'ing', 'f', 'ace', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = 'Ненавижу трансформеры, а особенно их скачивать с Hugging Face'\n",
    "text_encode = tokenizer.encode(text)\n",
    "print(text_encode.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "несколько предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ненавижу', 'трансформеры', '[SEP]', 'но', 'они', 'полез', 'ные', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text_encode = tokenizer.encode('Ненавижу трансформеры',\n",
    "                               'Но они полезные!')\n",
    "print(text_encode.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение токенизатора (словарь и правила слияния)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer_BPE\\\\vocab.json', 'tokenizer_BPE\\\\merges.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save('tokenizer_BPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение всего токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('Kinopoisk_tokenizer_BPE.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование токенизатора после сохранения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ненавижу', 'трансформеры', ',', 'а', 'особенно', 'их', 'ска', 'чивать', 'с', 'h', 'u', 'g', 'g', 'ing', 'f', 'ace', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizerFromFile = Tokenizer.from_file(\"Kinopoisk_tokenizer_BPE.json\")\n",
    "text_encode = tokenizerFromFile.encode(\"Ненавижу трансформеры, а особенно их скачивать с Hugging Face\")\n",
    "print(text_encode.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ненавижу', 'трансформеры', ',', 'а', 'особенно', 'их', 'ска', '##чивать', 'с', 'h', '##u', '##g', '##g', '##ing', 'f', '##ace']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece())\n",
    "tokenizer.normalizer = BertNormalizer() # очистка\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.decoder = WordPieceDecoder()\n",
    "\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size = 30000,\n",
    "    special_tokens = [\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"])\n",
    "\n",
    "tokenizer.train_from_iterator(data['text'].to_list(), trainer = trainer)\n",
    "output = tokenizer.encode(\"Ненавижу трансформеры, а особенно их скачивать с Hugging Face\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "проверка UNK токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[UNK]', 'm', '##ac', '##b', '##et', '##h', '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Kralsın aslansın Macbeth!\").tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
