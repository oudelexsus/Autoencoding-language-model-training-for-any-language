{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "from transformers import Trainer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–í 2003-–µ–º –≥–æ–¥—É –ø–æ–¥ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º –º–∞–ª–æ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dataset/neg/1000083-0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ì—Ä—É—Å—Ç–Ω–æ –∏ –ø–µ—á–∞–ª—å–Ω–æ. –ì—Ä—É—Å—Ç–Ω–æ –æ—Ç —Ç–æ–≥–æ, —á—Ç–æ –¥–æ–≤–æ–ª...</td>\n",
       "      <td>0</td>\n",
       "      <td>dataset/neg/1000083-1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–î–∞–≤–Ω—ã–º-–¥–∞–≤–Ω–æ –ö–∏—Ä–∞ –ù–∞–π—Ç–ª–∏ –≤–æ—Ä–≤–∞–ª–∞—Å—å –Ω–∞ —ç–∫—Ä–∞–Ω –æ—Ç...</td>\n",
       "      <td>0</td>\n",
       "      <td>dataset/neg/1000125-3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–Ø, –≤ –æ–±—â–µ–º, –Ω–∏—á–µ–≥–æ –ø—Ä–æ—Ç–∏–≤ —É—Ä–∞–≤–Ω–æ–≤–µ—à–µ–Ω–Ω–æ–≥–æ —Ñ–µ–º–∏...</td>\n",
       "      <td>0</td>\n",
       "      <td>dataset/neg/1000125-4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ò–∑–º–µ–Ω–∞  –æ–¥–∏–Ω –∏–∑ —Å—é–∂–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dataset/neg/1000125-6.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  –í 2003-–µ–º –≥–æ–¥—É –ø–æ–¥ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º –º–∞–ª–æ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ...      0   \n",
       "1  –ì—Ä—É—Å—Ç–Ω–æ –∏ –ø–µ—á–∞–ª—å–Ω–æ. –ì—Ä—É—Å—Ç–Ω–æ –æ—Ç —Ç–æ–≥–æ, —á—Ç–æ –¥–æ–≤–æ–ª...      0   \n",
       "2  –î–∞–≤–Ω—ã–º-–¥–∞–≤–Ω–æ –ö–∏—Ä–∞ –ù–∞–π—Ç–ª–∏ –≤–æ—Ä–≤–∞–ª–∞—Å—å –Ω–∞ —ç–∫—Ä–∞–Ω –æ—Ç...      0   \n",
       "3  –Ø, –≤ –æ–±—â–µ–º, –Ω–∏—á–µ–≥–æ –ø—Ä–æ—Ç–∏–≤ —É—Ä–∞–≤–Ω–æ–≤–µ—à–µ–Ω–Ω–æ–≥–æ —Ñ–µ–º–∏...      0   \n",
       "4  –ò–∑–º–µ–Ω–∞  –æ–¥–∏–Ω –∏–∑ —Å—é–∂–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç ...      0   \n",
       "\n",
       "                        link  \n",
       "0  dataset/neg/1000083-0.txt  \n",
       "1  dataset/neg/1000083-1.txt  \n",
       "2  dataset/neg/1000125-3.txt  \n",
       "3  dataset/neg/1000125-4.txt  \n",
       "4  dataset/neg/1000125-6.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ\n",
    "bert_tokenizer = BertWordPieceTokenizer()\n",
    "bert_tokenizer.train('corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ø—Ä–∏–≤–µ—Ç', '##—Å—Ç–≤—É—é', '–≤–∞—Å', '–≥—Ä–∞–∂–¥–∞–Ω–µ', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "tokenized_sentence = bert_tokenizer.encode('–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤–∞—Å –≥—Ä–∞–∂–¥–∞–Ω–µ !')\n",
    "tokenized_sentence.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "bert_tokenizer.save_model('tokenizer')\n",
    "tokenizer = BertWordPieceTokenizer.from_file(\"tokenizer/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '–ø—Ä–∏–≤–µ—Ç', '##—Å—Ç–≤—É—é', '–≤–∞—Å', '–≥—Ä–∞–∂–¥–∞–Ω–µ', '!', '[SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = tokenizer.encode('–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤–∞—Å –≥—Ä–∞–∂–¥–∞–Ω–µ !')\n",
    "tokenized_sentence.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oudel\\anaconda3\\envs\\newenv\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ –æ–±–æ–ª–æ—á–∫–µ transformers\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer\")\n",
    "\n",
    "# –æ–±–æ–ª–æ—á–∫–∞ LineByLine –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    file_path = \"corpus.txt\",\n",
    "    block_size = 128\n",
    ")\n",
    "\n",
    "# collator MLM –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –æ–±—É—á–µ–Ω–∏—é\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tokenizer,\n",
    "    mlm = True,\n",
    "    mlm_probability = 0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oudel\\anaconda3\\envs\\newenv\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moudelexsus2010\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\VSCODE__all\\NLP\\–ü–†–û–ï–ö–¢–´\\–ù–æ–≤–∞—è –ø–∞–ø–∫–∞ (3)\\Autoencoding-language-model-training-for-any-language\\wandb\\run-20240703_182232-szxh29su</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oudelexsus2010/huggingface/runs/szxh29su' target=\"_blank\">warm-glitter-36</a></strong> to <a href='https://wandb.ai/oudelexsus2010/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oudelexsus2010/huggingface' target=\"_blank\">https://wandb.ai/oudelexsus2010/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oudelexsus2010/huggingface/runs/szxh29su' target=\"_blank\">https://wandb.ai/oudelexsus2010/huggingface/runs/szxh29su</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8921bfe2eb400abe927db96dfacf6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.1263, 'grad_norm': 1.171692132949829, 'learning_rate': 2.5704567541302237e-05, 'epoch': 0.49}\n",
      "{'loss': 8.1207, 'grad_norm': 1.0261869430541992, 'learning_rate': 1.4091350826044704e-06, 'epoch': 0.97}\n",
      "{'train_runtime': 399.7437, 'train_samples_per_second': 329.396, 'train_steps_per_second': 2.574, 'train_loss': 8.605672586068467, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1029, training_loss=8.605672586068467, metrics={'train_runtime': 399.7437, 'train_samples_per_second': 329.396, 'train_steps_per_second': 2.574, 'train_loss': 8.605672586068467, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è \n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"BERT\",\n",
    "    overwrite_output_dir = True,\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 128\n",
    ")\n",
    "\n",
    "# —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ BERT —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (—Ç.–µ. –±–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏)\n",
    "tiny_bert_config = BertConfig(\n",
    "    max_position_embeddings=512,\n",
    "    hidden_size=128,\n",
    "    num_attention_heads=2,\n",
    "    num_hidden_layers=2,\n",
    "    intermediate_size=512\n",
    ")\n",
    "\n",
    "tiny_bert = BertForMaskedLM(tiny_bert_config)\n",
    "\n",
    "\n",
    "# –æ–±—ä–µ–∫—Ç Trainer\n",
    "trainer = Trainer(\n",
    "    model = tiny_bert,\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = dataset\n",
    ")\n",
    "\n",
    "# —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "trainer.save_model(\"MyBERT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
